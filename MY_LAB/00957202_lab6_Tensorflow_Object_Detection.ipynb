{"cells":[{"cell_type":"code","source":["#colab 刪除資料夾\n","import shutil\n","\n","! ls\n","shutil.rmtree('sample_data/')\n","! ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t9VDE-jQia2r","executionInfo":{"status":"ok","timestamp":1656109005786,"user_tz":-480,"elapsed":489,"user":{"displayName":"吳秉宸","userId":"02413005423567375094"}},"outputId":"73e7d621-d7f0-4831-d576-5cfcb7ce2f7b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["sample_data\n"]}]},{"cell_type":"code","source":["# https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_object_detection.ipynb#scrollTo=2JCeQU3fkayh\n","# This Colab requires TF 2.5.\n","\n","!pip install -U \"tensorflow>=2.5\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rfFFqAc6Cqh2","executionInfo":{"status":"ok","timestamp":1656109091195,"user_tz":-480,"elapsed":84111,"user":{"displayName":"吳秉宸","userId":"02413005423567375094"}},"outputId":"af1e1ac8-ab3d-4e1c-93ac-6c4d60eb0882"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow>=2.5 in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220527125636)\n","Collecting tensorflow>=2.5\n","  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n","\u001b[K     |████████████████████████████████| 511.7 MB 5.9 kB/s \n","\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5) (1.15.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5) (4.1.1)\n","Collecting keras<2.10.0,>=2.9.0rc0\n","  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 48.1 MB/s \n","\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5) (3.3.0)\n","Collecting gast<=0.4.0,>=0.2.1\n","  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5) (3.17.3)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5) (1.1.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5) (0.2.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5) (1.21.6)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5) (1.6.3)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5) (1.14.1)\n","Collecting flatbuffers<2,>=1.12\n","  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5) (3.1.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5) (1.1.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5) (0.26.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5) (1.46.3)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5) (1.1.2)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5) (14.0.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5) (21.3)\n","Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n","  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n","\u001b[K     |████████████████████████████████| 438 kB 67.0 MB/s \n","\u001b[?25hCollecting tensorboard<2.10,>=2.9\n","  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 41.3 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5) (57.4.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.5) (1.5.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5) (2.23.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5) (1.35.0)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5) (0.4.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5) (1.8.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5) (3.3.7)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.5) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.5) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.5) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.5) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.5) (4.11.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.5) (3.8.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.5) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.5) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.5) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.5) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.5) (2022.6.15)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.5) (3.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow>=2.5) (3.0.9)\n","Installing collected packages: tensorflow-estimator, tensorboard, keras, gast, flatbuffers, tensorflow\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.8.0\n","    Uninstalling tensorboard-2.8.0:\n","      Successfully uninstalled tensorboard-2.8.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.8.0\n","    Uninstalling keras-2.8.0:\n","      Successfully uninstalled keras-2.8.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.3\n","    Uninstalling gast-0.5.3:\n","      Successfully uninstalled gast-0.5.3\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 2.0\n","    Uninstalling flatbuffers-2.0:\n","      Successfully uninstalled flatbuffers-2.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n","    Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n","      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n","Successfully installed flatbuffers-1.12 gast-0.4.0 keras-2.9.0 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0\n"]}]},{"cell_type":"code","source":["import os\n","import pathlib\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","import io\n","import scipy.misc\n","import numpy as np\n","from six import BytesIO\n","from PIL import Image, ImageDraw, ImageFont\n","from six.moves.urllib.request import urlopen\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","\n","tf.get_logger().setLevel('ERROR')"],"metadata":{"id":"dXGlioMGCuTA","executionInfo":{"status":"ok","timestamp":1656109095109,"user_tz":-480,"elapsed":3921,"user":{"displayName":"吳秉宸","userId":"02413005423567375094"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# @title Run this!!\n","\n","def load_image_into_numpy_array(path):\n","  \"\"\"Load an image from file into a numpy array.\n","\n","  Puts image into numpy array to feed into tensorflow graph.\n","  Note that by convention we put it into a numpy array with shape\n","  (height, width, channels), where channels=3 for RGB.\n","\n","  Args:\n","    path: the file path to the image\n","\n","  Returns:\n","    uint8 numpy array with shape (img_height, img_width, 3)\n","  \"\"\"\n","  image = None\n","  if(path.startswith('http')):\n","    response = urlopen(path)\n","    image_data = response.read()\n","    image_data = BytesIO(image_data)\n","    image = Image.open(image_data)\n","  else:\n","    image_data = tf.io.gfile.GFile(path, 'rb').read()\n","    image = Image.open(BytesIO(image_data))\n","\n","  (im_width, im_height) = image.size\n","  return np.array(image.getdata()).reshape(\n","      (1, im_height, im_width, 3)).astype(np.uint8)\n","\n","\n","ALL_MODELS = {\n","'CenterNet HourGlass104 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512/1',\n","'CenterNet HourGlass104 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1',\n","'CenterNet HourGlass104 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024/1',\n","'CenterNet HourGlass104 Keypoints 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024_kpts/1',\n","'CenterNet Resnet50 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512/1',\n","'CenterNet Resnet50 V1 FPN Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512_kpts/1',\n","'CenterNet Resnet101 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet101v1_fpn_512x512/1',\n","'CenterNet Resnet50 V2 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512/1',\n","'CenterNet Resnet50 V2 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512_kpts/1',\n","'EfficientDet D0 512x512' : 'https://tfhub.dev/tensorflow/efficientdet/d0/1',\n","'EfficientDet D1 640x640' : 'https://tfhub.dev/tensorflow/efficientdet/d1/1',\n","'EfficientDet D2 768x768' : 'https://tfhub.dev/tensorflow/efficientdet/d2/1',\n","'EfficientDet D3 896x896' : 'https://tfhub.dev/tensorflow/efficientdet/d3/1',\n","'EfficientDet D4 1024x1024' : 'https://tfhub.dev/tensorflow/efficientdet/d4/1',\n","'EfficientDet D5 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d5/1',\n","'EfficientDet D6 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d6/1',\n","'EfficientDet D7 1536x1536' : 'https://tfhub.dev/tensorflow/efficientdet/d7/1',\n","'SSD MobileNet v2 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2',\n","'SSD MobileNet V1 FPN 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v1/fpn_640x640/1',\n","'SSD MobileNet V2 FPNLite 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1',\n","'SSD MobileNet V2 FPNLite 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_640x640/1',\n","'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_640x640/1',\n","'SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_1024x1024/1',\n","'SSD ResNet101 V1 FPN 640x640 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_640x640/1',\n","'SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_1024x1024/1',\n","'SSD ResNet152 V1 FPN 640x640 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_640x640/1',\n","'SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_1024x1024/1',\n","'Faster R-CNN ResNet50 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1',\n","'Faster R-CNN ResNet50 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_1024x1024/1',\n","'Faster R-CNN ResNet50 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_800x1333/1',\n","'Faster R-CNN ResNet101 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_640x640/1',\n","'Faster R-CNN ResNet101 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_1024x1024/1',\n","'Faster R-CNN ResNet101 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_800x1333/1',\n","'Faster R-CNN ResNet152 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_640x640/1',\n","'Faster R-CNN ResNet152 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_1024x1024/1',\n","'Faster R-CNN ResNet152 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_800x1333/1',\n","'Faster R-CNN Inception ResNet V2 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1',\n","'Faster R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_1024x1024/1',\n","'Mask R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1'\n","}\n","\n","IMAGES_FOR_TEST = {\n","  'Beach' : 'models/research/object_detection/test_images/image2.jpg',\n","  'Dogs' : 'models/research/object_detection/test_images/image1.jpg',\n","  # By Heiko Gorski, Source: https://commons.wikimedia.org/wiki/File:Naxos_Taverna.jpg\n","  'Naxos Taverna' : 'https://upload.wikimedia.org/wikipedia/commons/6/60/Naxos_Taverna.jpg',\n","  # Source: https://commons.wikimedia.org/wiki/File:The_Coleoptera_of_the_British_islands_(Plate_125)_(8592917784).jpg\n","  'Beatles' : 'https://upload.wikimedia.org/wikipedia/commons/1/1b/The_Coleoptera_of_the_British_islands_%28Plate_125%29_%288592917784%29.jpg',\n","  # By Américo Toledano, Source: https://commons.wikimedia.org/wiki/File:Biblioteca_Maim%C3%B3nides,_Campus_Universitario_de_Rabanales_007.jpg\n","  'Phones' : 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg/1024px-Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg',\n","  # Source: https://commons.wikimedia.org/wiki/File:The_smaller_British_birds_(8053836633).jpg\n","  'Birds' : 'https://upload.wikimedia.org/wikipedia/commons/0/09/The_smaller_British_birds_%288053836633%29.jpg',\n","}\n","\n","COCO17_HUMAN_POSE_KEYPOINTS = [(0, 1),\n"," (0, 2),\n"," (1, 3),\n"," (2, 4),\n"," (0, 5),\n"," (0, 6),\n"," (5, 7),\n"," (7, 9),\n"," (6, 8),\n"," (8, 10),\n"," (5, 6),\n"," (5, 11),\n"," (6, 12),\n"," (11, 12),\n"," (11, 13),\n"," (13, 15),\n"," (12, 14),\n"," (14, 16)]"],"metadata":{"id":"TDEUUeh2C2Kk","executionInfo":{"status":"ok","timestamp":1656108740510,"user_tz":-480,"elapsed":5,"user":{"displayName":"吳秉宸","userId":"02413005423567375094"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Clone the tensorflow models repository\n","!git clone --depth 1 https://github.com/tensorflow/models"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IUNDUspTC-rD","executionInfo":{"status":"ok","timestamp":1656109100163,"user_tz":-480,"elapsed":5064,"user":{"displayName":"吳秉宸","userId":"02413005423567375094"}},"outputId":"132695d2-4643-4697-bfba-d81879458415"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'models'...\n","remote: Enumerating objects: 3398, done.\u001b[K\n","remote: Counting objects: 100% (3398/3398), done.\u001b[K\n","remote: Compressing objects: 100% (2821/2821), done.\u001b[K\n","remote: Total 3398 (delta 896), reused 1411 (delta 520), pack-reused 0\u001b[K\n","Receiving objects: 100% (3398/3398), 34.94 MiB | 14.72 MiB/s, done.\n","Resolving deltas: 100% (896/896), done.\n"]}]},{"cell_type":"code","source":["%%bash\n","sudo apt install -y protobuf-compiler\n","cd models/research/\n","protoc object_detection/protos/*.proto --python_out=.\n","cp object_detection/packages/tf2/setup.py .\n","python -m pip install .\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7lc80NNIDAoA","executionInfo":{"status":"ok","timestamp":1656109159924,"user_tz":-480,"elapsed":59763,"user":{"displayName":"吳秉宸","userId":"02413005423567375094"}},"outputId":"eb83ec18-ed7d-421f-b34d-2a4662893ce1"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists...\n","Building dependency tree...\n","Reading state information...\n","protobuf-compiler is already the newest version (3.0.0-9.1ubuntu1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'sudo apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing /content/models/research\n","Collecting avro-python3\n","  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n","Collecting apache-beam\n","  Downloading apache_beam-2.39.0-cp37-cp37m-manylinux2010_x86_64.whl (10.3 MB)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (7.1.2)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (4.2.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (3.2.2)\n","Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.29.30)\n","Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.5.5)\n","Collecting tf-slim\n","  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.15.0)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.0.4)\n","Collecting lvis\n","  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.4.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.3.5)\n","Collecting tf-models-official>=2.5.1\n","  Downloading tf_models_official-2.9.2-py2.py3-none-any.whl (2.1 MB)\n","Collecting tensorflow_io\n","  Downloading tensorflow_io-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (25.9 MB)\n","Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.9.0)\n","Collecting pyparsing==2.4.7\n","  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n","Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n","Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.12.11)\n","Collecting opencv-python-headless\n","  Downloading opencv_python_headless-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (48.3 MB)\n","Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.0.1)\n","Collecting tensorflow-model-optimization>=0.4.1\n","  Downloading tensorflow_model_optimization-0.7.2-py2.py3-none-any.whl (237 kB)\n","Collecting pyyaml<6.0,>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","Collecting sacrebleu\n","  Downloading sacrebleu-2.1.0-py3-none-any.whl (92 kB)\n","Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.12)\n","Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n","Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n","Collecting py-cpuinfo>=3.3.0\n","  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.21.6)\n","Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.8)\n","Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","Collecting tensorflow-text~=2.9.0\n","  Downloading tensorflow_text-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","Requirement already satisfied: tensorflow~=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.9.1)\n","Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.17.4)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.1)\n","Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.0.4)\n","Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.31.6)\n","Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.35.0)\n","Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.23.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2022.1)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (21.3)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.56.2)\n","Requirement already satisfied: protobuf<4.0.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.17.3)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (57.4.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.8)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2022.6.15)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (6.1.2)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.8.2)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.24.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.64.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.26.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (4.1.1)\n","Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (2.9.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.46.3)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (14.0.1)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.2)\n","Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (2.9.1)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.14.1)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n","Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.12)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.5.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.7)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (4.11.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (3.8.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (3.2.0)\n","Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.7)\n","Collecting orjson<4.0\n","  Downloading orjson-3.7.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (256 kB)\n","Collecting proto-plus<2,>=1.7.1\n","  Downloading proto_plus-1.20.6-py3-none-any.whl (46 kB)\n","Collecting fastavro<2,>=0.23.6\n","  Downloading fastavro-1.5.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n","Requirement already satisfied: pyarrow<8.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (6.0.1)\n","Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n","Collecting cloudpickle<3,>=2.0.0\n","  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n","Collecting dill<0.3.2,>=0.3.1.1\n","  Downloading dill-0.3.1.1.tar.gz (151 kB)\n","Collecting pymongo<4.0.0,>=3.8.0\n","  Downloading pymongo-3.12.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (508 kB)\n","Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n","Collecting hdfs<3.0.0,>=2.1.0\n","  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n","Collecting requests<3.0.0dev,>=2.18.0\n","  Downloading requests-2.28.0-py3-none-any.whl (62 kB)\n","Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1) (0.6.2)\n","Collecting protobuf<4.0.0dev,>=3.12.0\n","  Downloading protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.0.12)\n","Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (0.11.0)\n","Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (4.1.2.30)\n","Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (1.4.3)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n","Collecting portalocker\n","  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n","Collecting colorama\n","  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (0.8.9)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (2022.6.2)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official>=2.5.1->object-detection==0.1) (2.7.1)\n","Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (21.4.0)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.8.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (5.7.1)\n","Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.16.0)\n","Building wheels for collected packages: object-detection, py-cpuinfo, dill, avro-python3, seqeval\n","  Building wheel for object-detection (setup.py): started\n","  Building wheel for object-detection (setup.py): finished with status 'done'\n","  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1694695 sha256=7c78be441c5e20948bec01d0abc8f7ac8c84ff6d76330a6d76cee4e2ae78b1e2\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-0cxdu4wn/wheels/fa/a4/d2/e9a5057e414fd46c8e543d2706cd836d64e1fcd9eccceb2329\n","  Building wheel for py-cpuinfo (setup.py): started\n","  Building wheel for py-cpuinfo (setup.py): finished with status 'done'\n","  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=2e94d4ca5f2c70de02f8cfb7ccb895421b3073bb8c94afb2ee48d6758090d5ec\n","  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n","  Building wheel for dill (setup.py): started\n","  Building wheel for dill (setup.py): finished with status 'done'\n","  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=3cb80697796208664d9c5822e1fe2215f4962a9ca3ae07f7c60bbe4d92081cbe\n","  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n","  Building wheel for avro-python3 (setup.py): started\n","  Building wheel for avro-python3 (setup.py): finished with status 'done'\n","  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=44010 sha256=5efa0a6413875fe22f7170b63c1ac3e8e32d6fd7a53380bd7d826fd2dc7bbe62\n","  Stored in directory: /root/.cache/pip/wheels/d6/e5/b1/6b151d9b535ee50aaa6ab27d145a0104b6df02e5636f0376da\n","  Building wheel for seqeval (setup.py): started\n","  Building wheel for seqeval (setup.py): finished with status 'done'\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=ffa6d8e864b50ce8e4d67901b1ef97dd3dc425d0d647278dc218cdd33d7b4742\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built object-detection py-cpuinfo dill avro-python3 seqeval\n","Installing collected packages: requests, pyparsing, protobuf, portalocker, dill, colorama, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, pymongo, py-cpuinfo, proto-plus, orjson, opencv-python-headless, hdfs, fastavro, cloudpickle, tf-models-official, tensorflow-io, lvis, avro-python3, apache-beam, object-detection\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Attempting uninstall: pyparsing\n","    Found existing installation: pyparsing 3.0.9\n","    Uninstalling pyparsing-3.0.9:\n","      Successfully uninstalled pyparsing-3.0.9\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.17.3\n","    Uninstalling protobuf-3.17.3:\n","      Successfully uninstalled protobuf-3.17.3\n","  Attempting uninstall: dill\n","    Found existing installation: dill 0.3.5.1\n","    Uninstalling dill-0.3.5.1:\n","      Successfully uninstalled dill-0.3.5.1\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: pymongo\n","    Found existing installation: pymongo 4.1.1\n","    Uninstalling pymongo-4.1.1:\n","      Successfully uninstalled pymongo-4.1.1\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 1.3.0\n","    Uninstalling cloudpickle-1.3.0:\n","      Successfully uninstalled cloudpickle-1.3.0\n","Successfully installed apache-beam-2.39.0 avro-python3-1.10.2 cloudpickle-2.1.0 colorama-0.4.5 dill-0.3.1.1 fastavro-1.5.1 hdfs-2.7.0 lvis-0.5.3 object-detection-0.1 opencv-python-headless-4.6.0.66 orjson-3.7.3 portalocker-2.4.0 proto-plus-1.20.6 protobuf-3.19.4 py-cpuinfo-8.0.0 pymongo-3.12.3 pyparsing-2.4.7 pyyaml-5.4.1 requests-2.28.0 sacrebleu-2.1.0 sentencepiece-0.1.96 seqeval-1.2.2 tensorflow-addons-0.17.1 tensorflow-io-0.26.0 tensorflow-model-optimization-0.7.2 tensorflow-text-2.9.0 tf-models-official-2.9.2 tf-slim-1.1.0\n"]},{"output_type":"stream","name":"stderr","text":["\n","WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n","\n","  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\n","ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","multiprocess 0.70.13 requires dill>=0.3.5.1, but you have dill 0.3.1.1 which is incompatible.\n","gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.1.0 which is incompatible.\n","google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.28.0 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\n"]}]},{"cell_type":"markdown","metadata":{"id":"btYdSfXU9rSA"},"source":["#  <span style=\"color:blue\"> Tensorflow Object Detection API</span>\n","\n","https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/\n","\n","https://github.com/tensorflow/models\n","\n","\n","#### slim\n","https://github.com/google-research/tf-slim\n","\n","### 安裝額外package\n","<pre>\n","cd research\n","\n","pip install .\n","\n","conda -c conda-forge pycocotools\n","</pre>\n","\n","## <font color=\"purple\">下載與安裝Tensorflow Object Detection API於Windows 10與Anaconda</font>\n","在官方<a href=\"https://github.com/tensorflow/models\" target=\"_blank\" title =\"Tensorflow object detection\">文件</a>或<a href=\"https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\" target=\"_blank\" title=\"object_detection_tutorial.jpynb\">object_detection_tutorial.jpynb</a>有說明安裝在MacOS與Linux作業系統的步驟(比較單純)。這裡著重安裝在Windows 10與Anaconda上的步驟。\n","\n","### 步驟\n","1. 從https://github.com/tensorflow/models 下載models-master.zip並解壓縮至你要的路徑(假設為models)\n","2. 從https://github.com/protocolbuffers/protobuf/releases 下載protoc-3.11.3-win64.zip。\n","3. 將protoc-3.11.3-win64.zip裡bin\\protoc.exe解壓到models\\research，因此models\\research\\bin內有protoc.exe這個檔。\n","4. 對每一個在models\\research\\object_detection\\protos下的.proto檔轉出相對應的.py檔。可以在models\\research下用文字編輯器建立一個內容如下的cvt.bat檔，然後再執行cvt.bat。\n","<pre>\n","bin\\protoc object_detection\\protos\\anchor_generator.proto --python_out=.\n","bin\\protoc object_detection\\protos\\argmax_matcher.proto --python_out=.\n","bin\\protoc object_detection\\protos\\bipartite_matcher.proto --python_out=.\n","bin\\protoc object_detection\\protos\\box_coder.proto --python_out=.\n","bin\\protoc object_detection\\protos\\box_predictor.proto --python_out=.\n","bin\\protoc object_detection\\protos\\calibration.proto --python_out=.\n","bin\\protoc object_detection\\protos\\eval.proto --python_out=.\n","bin\\protoc object_detection\\protos\\faster_rcnn.proto --python_out=.\n","bin\\protoc object_detection\\protos\\faster_rcnn_box_coder.proto --python_out=.\n","bin\\protoc object_detection\\protos\\flexible_grid_anchor_generator.proto --python_out=.\n","bin\\protoc object_detection\\protos\\graph_rewriter.proto --python_out=.\n","bin\\protoc object_detection\\protos\\grid_anchor_generator.proto --python_out=.\n","bin\\protoc object_detection\\protos\\hyperparams.proto --python_out=.\n","bin\\protoc object_detection\\protos\\image_resizer.proto --python_out=.\n","bin\\protoc object_detection\\protos\\input_reader.proto --python_out=.\n","bin\\protoc object_detection\\protos\\keypoint_box_coder.proto --python_out=.\n","bin\\protoc object_detection\\protos\\losses.proto --python_out=.\n","bin\\protoc object_detection\\protos\\matcher.proto --python_out=.\n","bin\\protoc object_detection\\protos\\mean_stddev_box_coder.proto --python_out=.\n","bin\\protoc object_detection\\protos\\model.proto --python_out=.\n","bin\\protoc object_detection\\protos\\multiscale_anchor_generator.proto --python_out=.\n","bin\\protoc object_detection\\protos\\optimizer.proto --python_out=.\n","bin\\protoc object_detection\\protos\\pipeline.proto --python_out=.\n","bin\\protoc object_detection\\protos\\post_processing.proto --python_out=.\n","bin\\protoc object_detection\\protos\\preprocessor.proto --python_out=.\n","bin\\protoc object_detection\\protos\\region_similarity_calculator.proto --python_out=.\n","bin\\protoc object_detection\\protos\\square_box_coder.proto --python_out=.\n","bin\\protoc object_detection\\protos\\ssd.proto --python_out=.\n","bin\\protoc object_detection\\protos\\ssd_anchor_generator.proto --python_out=.\n","bin\\protoc object_detection\\protos\\string_int_label_map.proto --python_out=.\n","bin\\protoc object_detection\\protos\\target_assigner.proto --python_out=.\n","bin\\protoc object_detection\\protos\\train.proto --python_out=.\n","</pre>\n","5. 開命令列視窗在models\\research目錄下執行指令 (注意虛擬環境設定)\n","\n","       pip install .\n","       \n","6. 若安裝成功，應該可以正確執行下面例子。"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"i7anskva9rSI","executionInfo":{"status":"ok","timestamp":1656109160375,"user_tz":-480,"elapsed":461,"user":{"displayName":"吳秉宸","userId":"02413005423567375094"}}},"outputs":[],"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","from PIL import Image\n","from IPython.display import display\n","\n","from object_detection.utils import ops as utils_ops\n","from object_detection.utils import label_map_util\n","from object_detection.utils import visualization_utils as vis_util\n","\n","\n","# patch tf1 into `utils.ops`\n","utils_ops.tf = tf.compat.v1\n","\n","# Patch the location of gfile\n","tf.gfile = tf.io.gfile\n","\n","def load_model(model_name):\n","    base_url   = 'http://download.tensorflow.org/models/object_detection/'\n","    model_file = model_name + '.tar.gz'\n","    model_dir  = tf.keras.utils.get_file(fname=model_name, origin=base_url + model_file, untar=True)\n","    model_dir  = os.path.join(model_dir,\"saved_model\")\n","    model      = tf.saved_model.load(model_dir)\n","    model      = model.signatures['serving_default']\n","    return model\n","\n","def run_inference_for_single_image(model, image):\n","    \n","    image = np.asarray(image)\n","    \n","  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n","    input_tensor = tf.convert_to_tensor(image)\n","    \n","  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n","    input_tensor = input_tensor[tf.newaxis,...]\n","\n","  # Run inference\n","    output_dict = model(input_tensor)\n","\n","  # All outputs are batches tensors.\n","  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n","  # We're only interested in the first num_detections.\n","    num_detections = int(output_dict.pop('num_detections'))\n","    output_dict = {key:value[0, :num_detections].numpy() for key,value in output_dict.items()}\n","    output_dict['num_detections'] = num_detections\n","\n","  # detection_classes should be ints.\n","    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n","   \n","  # Handle models with masks:\n","    if 'detection_masks' in output_dict:\n","    # Reframe the the bbox mask to the image size.\n","        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n","              output_dict['detection_masks'], output_dict['detection_boxes'],\n","               image.shape[0], image.shape[1])      \n","        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n","                                       tf.uint8)\n","        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n","    \n","    return output_dict\n","\n","def show_inference(model, image_path):\n","  # the array based representation of the image will be used later in order to prepare the\n","  # result image with boxes and labels on it.\n","    image_np = np.array(Image.open(image_path))\n","    print(image_np.shape)\n","  # Actual detection.\n","    output_dict = run_inference_for_single_image(model, image_np)=\n","  # Visualization of the results of a detection.\n","    vis_util.visualize_boxes_and_labels_on_image_array(\n","      image_np,\n","      output_dict['detection_boxes'],\n","      output_dict['detection_classes'],\n","      output_dict['detection_scores'],\n","      category_index,\n","      instance_masks=output_dict.get('detection_masks_reframed', None),\n","      use_normalized_coordinates=True,\n","      line_thickness=8)\n","\n","    display(Image.fromarray(image_np))"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"hQhHSIuw9rSK","executionInfo":{"status":"ok","timestamp":1656109160375,"user_tz":-480,"elapsed":5,"user":{"displayName":"吳秉宸","userId":"02413005423567375094"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e3923663-4e97-4221-b194-8c5930ba931d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/mscoco_label_map.pbtxt\n","5056/5056 [==============================] - 0s 0us/step\n"]}],"source":["# load label names\n","PATH_TO_LABELS = tf.keras.utils.get_file(fname='mscoco_label_map.pbtxt', origin='https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/mscoco_label_map.pbtxt',cache_subdir='datasets/tensorflow/object_detection/data')\n","category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"jKjE1-p59rSK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656109161008,"user_tz":-480,"elapsed":637,"user":{"displayName":"吳秉宸","userId":"02413005423567375094"}},"outputId":"efeba163-7e74-4c9a-b1a7-7e5d556fa11c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/test_images/image1.jpg\n","129862/129862 [==============================] - 0s 0us/step\n","Downloading data from https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/test_images/image2.jpg\n","1415684/1415684 [==============================] - 0s 0us/step\n"]}],"source":["#load test images from the offical website\n","test_image_url = ['https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/test_images/image1.jpg',\n","                  'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/test_images/image2.jpg']  \n","TEST_IMAGE_PATHS=[]\n","for url in test_image_url:\n","    TEST_IMAGE_PATHS.append(tf.keras.utils.get_file(fname=url.split('/')[-1], origin=url,cache_subdir='datasets/tensorflow/object_detection/test_images'))  \n","# you can add the filenames of more test images to TEST_IMAGE_PATHS   "]},{"cell_type":"code","source":["train_x = np.array([np.array(Image.open(\"vid_4_600.jpg\")),np.array(Image.open(\"vid_4_600.jpg\"))])\n","print(train_x.shape)\n","train_y = np.array([np.array([286.6397,187.5241,407.9479,232.0286]),np.array([286.6397,187.5241,407.9479,232.0286])])\n","print(train_y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"5TSjkrWKSvLJ","executionInfo":{"status":"error","timestamp":1656094214633,"user_tz":-480,"elapsed":276,"user":{"displayName":"吳秉宸","userId":"02413005423567375094"}},"outputId":"96f2651a-ecc0-4204-b60d-394bd4212413"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-3b9d5f495f2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vid_4_600.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vid_4_600.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m286.6397\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m187.5241\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m407.9479\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m232.0286\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m286.6397\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m187.5241\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m407.9479\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m232.0286\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vid_4_600.jpg'"]}]},{"cell_type":"code","source":["# conv1 =\n","# conv1 = tf.keras.layers.BatchNormalization()(conv1)\n","# tf.keras.layers.Conv2D(256, (1, 1), padding='same', activation='relu')\n","my_model = tf.keras.models.Sequential()\n","my_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(\n","    3, 3), padding=\"same\", activation='relu', input_shape=train_x.shape[1:]))  # [380, 676, 3]\n","my_model.add(tf.keras.layers.BatchNormalization())\n","my_model.add(tf.keras.layers.Conv2D(\n","    16, (5, 5), dilation_rate=6, padding='same', activation='relu'))\n","# my_model.add(tf.keras.layers.Conv2D(\n","#     64, (3, 3), dilation_rate=18, padding='same', activation='relu'))\n","# my_model.add(tf.keras.layers.Conv2D(\n","#     64, (3, 3), padding='same', activation='relu'))\n","# my_model.add(tf.keras.layers.GlobalAveragePooling1D())\n","my_model.add(tf.keras.layers.Flatten())\n","my_model.add(tf.keras.layers.Dense(4, activation='relu'))\n","# my_model.add(tf.keras.layers.MaxPooling2D((2, 2), (2, 2), padding='same'))\n","\n","\n","my_model.compile(loss='categorical_crossentropy',\n","                 optimizer='adam', metrics=['accuracy'])\n","history = my_model.fit(train_x, train_y, batch_size=10, epochs=10)"],"metadata":{"id":"melrHhs9I8Zu","colab":{"base_uri":"https://localhost:8080/","height":312},"executionInfo":{"status":"error","timestamp":1656094173497,"user_tz":-480,"elapsed":6,"user":{"displayName":"吳秉宸","userId":"02413005423567375094"}},"outputId":"360a9132-3172-4d67-d829-c506b1a05671"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b1af8036b7b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# conv1 = tf.keras.layers.BatchNormalization()(conv1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# tf.keras.layers.Conv2D(256, (1, 1), padding='same', activation='relu')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrnn_model2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdrnn_model2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"same\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m380\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m676\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdrnn_model2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"]}]},{"cell_type":"code","source":["# 線性堆疊模型\n","model = Sequential()\n","\n","# 建立卷積層1\n","# 輸入數字影像 28x28 的大小，執行一次卷積運算，產生 16 個影像，卷積運算不會改變影像大小，結果還是 28x28\n","# filters=16             建立 16 個 filter weight\n","# kernel_size=(5,5)      每一個濾鏡大小為 5x5\n","# padding='same'         讓卷積運算產生的影像大小不變\n","# input_shape=(28,28,1)  第1, 2 維，是輸入的影像形狀 28x28，第 3 維，因為是單色灰階影像，所以是 1\n","# activation='relu'      設定 ReLU 激活函數\n","model.add(Conv2D(filters=16,\n","                 kernel_size=(5,5),\n","                 padding='same',\n","                 input_shape=(28,28,1),\n","                 activation='relu'))\n","\n","# 建立池化層\n","# 輸入參數 pool_size=(2, 2)，執行第一次縮減取樣，將 16 個 28x28 影像，縮小為 16 個 14x14 的影像\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","# 建立卷積層2\n","# 執行第二次卷積運算，將原本的 16 個影像，轉換為 36 個影像，卷積運算不會改變影像大小，結果還是 14x14\n","model.add(Conv2D(filters=36,\n","                 kernel_size=(5,5),\n","                 padding='same',\n","                 activation='relu'))\n","\n","# 建立池化層2，並加入Dropout 避免 overfitting\n","# 執行第二次縮減取樣，將 36 個 14x14 的影像，縮小為 36 個 7x7 的影像\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","# 建立神經網路 (平坦層, 隱藏層, 輸出層)\n","# 建立平坦層\n","# 根據池化層2 的結果，共36 個 7x7 影像，轉換為 1維向量，長度是 36x7x7=1764，也就是 1764 個 float，正好對應到 1764 個神經元\n","model.add(Flatten())\n","# 建立隱藏層，共有 128 個神經元\n","model.add(Dense(128, activation='relu'))\n","# 加入 Dropout(0.5)\n","# 每次訓練迭代時，會隨機在神經網路中，放棄 50% 的神經元，以避免 overfitting\n","model.add(Dropout(0.5))\n","# 建立輸出層\n","# 共 10 個神經元，對應 0~9 共 10 個數字，並使用 softmax 激活函數進行轉換\n","# softmax 可將神經元的輸出，轉換為預測每一個數字的機率\n","model.add(Dense(10,activation='softmax'))\n"],"metadata":{"id":"NjMKsp5WkQZA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ASPPBlock(tf.keras.Model):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = tf.keras.layers.Conv2D(256, (1, 1), padding='same', activation='relu')\n","        self.conv2 = tf.keras.layers.Conv2D(256, (3, 3), dilation_rate=6, padding='same', activation='relu')\n","        self.conv3 = tf.keras.layers.Conv2D(256, (3, 3), dilation_rate=12, padding='same', activation='relu')\n","        self.conv4 = tf.keras.layers.Conv2D(256, (3, 3), dilation_rate=18, padding='same', activation='relu')\n","        self.conv5 = tf.keras.layers.Conv2D(256, (1, 1), padding='same', activation='relu')\n","\n","    def call(self, inp, is_training=False):\n","        out1 = self.conv1(inp)\n","        out2 = self.conv2(inp)\n","        out3 = self.conv3(inp)\n","        out4 = self.conv4(inp)\n","        out = tf.concat([out1, out2, out3, out4], axis=3)\n","        out = self.conv5(out)\n","        return out\n","    \n","class ASPPNet(tf.keras.Model):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu')\n","        self.conv2 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu')\n","        self.conv3 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu')\n","        self.conv4 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu')\n","        self.conv5 = tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu')\n","        self.conv6 = tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu')\n","        self.conv7 = tf.keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu')\n","        self.conv8 = tf.keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu')\n","        self.conv9 = tf.keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu')\n","        self.conv10 = tf.keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu')\n","\n","        self.conv11 = tf.keras.layers.Conv2D(48, (1, 1), padding='same', activation='relu')\n","        self.conv12 = tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu')\n","        self.conv13 = tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu')\n","        self.conv14 = tf.keras.layers.Conv2D(1, (1, 1), padding='same', activation=None)\n","\n","        self.maxpool = tf.keras.layers.MaxPooling2D((2, 2), (2, 2), padding='same')\n","\n","        self.aspp = ASPPBlock()\n","\n","    def call(self, x):\n","\n","        out = self.conv1(x)\n","        out = self.conv2(out)\n","        out = self.maxpool(out)\n","        out = self.conv3(out)\n","        out = self.conv4(out)\n","        out = self.maxpool(out)\n","        out = self.conv5(out)\n","        out = self.conv6(out)\n","        out_enc_mid = out\n","        out = self.maxpool(out)\n","        out = self.conv7(out)\n","        out = self.conv8(out)\n","        out = self.maxpool(out)\n","        out = self.conv9(out)\n","        out = self.conv10(out)\n","\n","        out = self.aspp(out)\n","\n","        out = tf.image.resize(out, tf.shape(out_enc_mid)[1:3], tf.image.ResizeMethod.BILINEAR)\n","\n","        out_enc_mid = self.conv11(out_enc_mid)\n","\n","        out = tf.concat([out, out_enc_mid], axis=3)\n","\n","        out = self.conv12(out)\n","        out = self.conv13(out)\n","        out = self.conv14(out)\n","\n","        out = tf.image.resize(out, tf.shape(x)[1:3], tf.image.ResizeMethod.BILINEAR)\n","        out = tf.nn.sigmoid(out)\n","        return out\n","    \n","model = ASPPNet()\n","loss = tf.keras.losses.BinaryCrossentropy()\n","optimizer = tf.keras.optimizers.Adam()\n","model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n","model.build(input_shape=[None, 256, 256, 3])\n","\n","history = model.fit(train_x, train_y, verbose=2, epochs=10, steps_per_epoch=25)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Ib3Ka05kdF3g","executionInfo":{"status":"error","timestamp":1656086722864,"user_tz":-480,"elapsed":725,"user":{"displayName":"吳秉宸","userId":"02413005423567375094"}},"outputId":"f5cc4129-3f7c-4a96-8de7-f9eeaac73ecc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-75-e0faebef6c26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/__autograph_generated_file_b45opw0.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file_b45opw0.py\", line 10, in tf__call\n        out = ag__.converted_call(ag__.ld(self).conv1, (ag__.ld(x),), None, fscope)\n\n    TypeError: Exception encountered when calling layer \"aspp_net_4\" (type ASPPNet).\n    \n    in user code:\n    \n        File \"<ipython-input-74-64c324ccdc7b>\", line 44, in call  *\n            out = self.conv1(x)\n        File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        TypeError: Exception encountered when calling layer \"conv2d_220\" (type Conv2D).\n        \n        Value passed to parameter 'input' has DataType uint8 not in list of allowed values: float16, bfloat16, float32, float64, int32\n        \n        Call arguments received by layer \"conv2d_220\" (type Conv2D):\n          • inputs=tf.Tensor(shape=(1, 380, 676, 3), dtype=uint8)\n    \n    \n    Call arguments received by layer \"aspp_net_4\" (type ASPPNet):\n      • x=tf.Tensor(shape=(1, 380, 676, 3), dtype=uint8)\n"]}]},{"cell_type":"markdown","metadata":{"id":"ztkIDZRB9rSK"},"source":["### Object Detection"]},{"cell_type":"code","source":["\n","drnn_model2 = tf.keras.models.Sequential()\n","drnn_model2.add(tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape =[None, 3]) )\n","drnn_model2.add(tf.keras.layers.SimpleRNN(72, return_sequences=True) )\n","drnn_model2.add(tf.keras.layers.SimpleRNN(72))\n","drnn_model2.add(tf.keras.layers.Dense(4))\n","\n"],"metadata":{"id":"KL9EzLrjHdaq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"id":"REatQs4p9rSL","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1bvUIziHLL_iKFgq66j6OLoZ9QGKXmLPx"},"executionInfo":{"status":"ok","timestamp":1656109197750,"user_tz":-480,"elapsed":36744,"user":{"displayName":"吳秉宸","userId":"02413005423567375094"}},"outputId":"6119fb36-918c-4e06-d155-5cb1d3acd9f2"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["detection_model = load_model('ssd_mobilenet_v2_coco_2018_03_29')\n","\n","print(detection_model.inputs)\n","print(detection_model.output_dtypes)\n","\n","for image_path in TEST_IMAGE_PATHS:\n","    show_inference(detection_model, image_path)"]},{"cell_type":"markdown","metadata":{"id":"h6qL1cG59rSM"},"source":["### Instance Segmentation"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Kbn-kNnS9rSM","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1DC5ZJ_Il9uaLB9yAxS-qpJ4BjcfzdcBQ"},"executionInfo":{"status":"ok","timestamp":1656109374921,"user_tz":-480,"elapsed":177185,"user":{"displayName":"吳秉宸","userId":"02413005423567375094"}},"outputId":"14229684-4fec-43ff-b436-8c3a4dd40e96"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["masking_model = load_model('mask_rcnn_inception_resnet_v2_atrous_coco_2018_01_28')\n","print(masking_model.output_shapes)\n","\n","for image_path in TEST_IMAGE_PATHS:\n","    show_inference(masking_model, image_path)"]},{"cell_type":"markdown","metadata":{"id":"rOTBqDfD9rSN"},"source":["## <font color=\"purple\">安裝影像標註工具(Annotation Tools)</font>\n","\n","### 工具:labelImg\n","可標註目標物範圍矩形區域(bounding box)，適合用來標註物件偵測器的訓練影像，輸出格式可為PASCAL VOC與YOLO格式。\n","#### labelImg安裝\n","+ <a href=\"https://github.com/tzutalin/labelImg\" target=\"_blank\" title=\"labelImg\">labelImg下載</a>\n","+ <a href=\"https://github.com/tzutalin/labelImg#windows--anaconda\" target=\"_blank\" title=\"labelImg on Windows 10 + Anaconda\">在Windows10與Anaconda安裝labelImg</a>\n","  + <a href=\"https://anaconda.org/anaconda/pyqt\" target=\"_blank\" title=\"Qt Installation\">安裝PyQt</a>。\n","  + 在命令列視窗裡，labelImg的目錄下(這個目錄裡有resources.qrc這個檔案)執行下面指令，將labelImg的.qrc資源檔轉成.py檔。\n","    \n","          pyrcc5 -o libs/resources.py resources.qrc\n","          \n","#### labelImg執行          \n","在命令列視窗裡，labelImg的目錄下執行下面指令，就可以啟動labelImg。\n","    \n","          python labelImg.py [IMAGE_PATH] [PRE-DEFINED CLASS FILE]\n","+ 善用labelImg的<a href=\"https://github.com/tzutalin/labelImg#hotkeys\" target=\"_blank\" title=\"Hot Keys\">快速鍵</a>。  \n","+ 解說labelImg使用的<a href=\"https://youtu.be/K_mFnvzyLvc?t=9m13s\" target=\"_blank\" title=\"labelImg\">視訊</a>。\n","<div>\n","    <img src=\"attachment:LabelImg.png\" width=\"400\" align=\"left\">\n","</div>    "]},{"cell_type":"markdown","metadata":{"id":"5kjcScsO9rSO"},"source":["## <font color=\"purple\">訓練自己的物件偵測器</font>\n","詳細步驟請參考這份<a href=\"https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html\" target=\"_blank\" title=\"Training Custom Object Detector\">文件</a>。將標註工具(labelImg)、上面安裝的模組(models)、訓練資料集(fish-dataset)安排成下面資料夾結構:\n","<pre>\n","TensorFlow\n","├─ addons\n","│   └─ labelImg\n","├─ models\n","│   ├─ official\n","│   ├─ research\n","│   ├─ samples\n","│   └─ tutorials\n","├─ scripts\n","│   └─ preprocessing\n","└─ workspace\n","    └─ fish_dataset\n","        ├─ annotations\n","        ├─ images\n","        │   ├─ test\n","        │   └─ train\n","        ├─ pre-trained-model\n","        └─ training\n","</pre>\n","\n","步驟摘要說明:\n","\n","### 步驟1:準備訓練資料\n","訓練影像若放在fish-dataset資料夾裡，安排其子資料夾如下目錄結構:\n","<pre>\n","fish-dataset\n","├─ annotations\n","├─ images\n","│   ├─ test\n","│   └─ train\n","├─ pre-trained-model\n","└─ training\n","</pre>\n","\n","  + annotations:將存放有關標註資訊的.csv檔與相對應*.record檔(TFRecord)。\n","\n","  + images:\n","\n","    + images\\train:所有訓練影像及其相對應的.xml檔(若影像已標示)複製到這個資料夾。\n","  \n","    + images\\test:所有測試影像及其相對應的.xml檔(若影像已標示)複製到這個資料夾\n","  \n","  + pre-trained-model:將擺放pretrain model及checkpoint之用。\n","\n","  + training:將擺放訓練設定參數的.config設定檔及一個標籤流水號與物件名稱對應檔.pbtxt。\n","  \n","### 步驟2:建立標籤流水號與物件名稱對映檔label_map.pbtxt並存放在annotations資料夾。\n","此例標籤流水號與物件名稱對映檔，放在annotations資料夾檔名為label_map.pbtxt內容如下範例:\n","<pre>\n","item {\n","    id: 1\n","    name: 'salman'\n","}\n","item {\n","    id: 2\n","    name: 'swordfish'\n","}\n","</pre>\n","\n","### 步驟3:標記影像\n","使用labelImg或其他工具標記目標物在影像內的位置。  \n","\n","### 步驟4:將標註資訊轉換成TFRecord格式\n","\n","官網準備兩個轉檔程式，將它們存到scripts/preprocessing下，照著做，再改一點程式應該會成功。\n","+ <a href=\"https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#converting-xml-to-csv\"> xml_to_csv.py</a>\n","+ <a href=\"https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#converting-from-csv-to-record\"> generate_tfrecord.py</a>\n","\n","下面有一個簡化與整合的程式，將它存在xml_to_tfrecord.py (放在train、test下打指令比較快)。\n","在命令列，切換到train目錄下執行它。\n","<pre>\n","     python xml_to_tfrecord.py\n","</pre>\n","那麼在annotations目錄內會產生train.record、label_map.pdtxt。若切換到test目錄執行\n","<pre>\n","     python xml_to_tfrecord.py\n","</pre>\n","那麼在annotations目錄內會產生test.record、label_map.pdtxt。"]},{"cell_type":"markdown","metadata":{"id":"ZSy8-f6w9rSO"},"source":["<pre>\n","\"\"\"\n","Usage:\n","# Create train data:\n","python xml_to_tfrecord.py -i [PATH_TO_IMAGES_FOLDER]/train -o [PATH_TO_ANNOTATIONS_FOLDER]/train.record -l [PATH_TO_ANNOTATIONS_FOLDER]/label_map_file.pbtxt\n","or\n","# Create train.record and label_map.pbtxt in [PATH_TO_ANNOTATIONS_FOLDER]\n","python xml_to_tfrecord.py \n","\n","# Create test data:\n","python xml_to_tfrecord.py -i [PATH_TO_IMAGES_FOLDER]/test -o [PATH_TO_ANNOTATIONS_FOLDER]/test.record -l [PATH_TO_ANNOTATIONS_FOLDER]/label_map_file.pbtxt\n","or\n","# Create train.record and label_map.pbtxt in [PATH_TO_ANNOTATIONS_FOLDER]\n","python xml_to_tfrecord.py \n","\"\"\"\n","from __future__ import division,print_function,absolute_import\n","\n","import os\n","import glob\n","import pandas as pd\n","import argparse\n","import xml.etree.ElementTree as ET\n","import io\n","import tensorflow as tf\n","from PIL import Image\n","from object_detection.utils import dataset_util\n","from collections import namedtuple, OrderedDict\n","\n","def xml_to_csv(path):\n","    xml_list = []\n","    for xml_file in glob.glob(path + '/*.xml'):\n","        tree = ET.parse(xml_file)\n","        root = tree.getroot()\n","        for member in root.findall('object'):\n","            value = (root.find('filename').text,\n","                    int(root.find('size')[0].text),\n","                    int(root.find('size')[1].text),\n","                    member[0].text,\n","                    int(member[4][0].text),\n","                    int(member[4][1].text),\n","                    int(member[4][2].text),\n","                    int(member[4][3].text)\n","                    )\n","            xml_list.append(value)\n","    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n","    xml_df      = pd.DataFrame(xml_list, columns=column_name)\n","    return xml_df\n"," \n","  \n","def split(df, group):\n","    data = namedtuple('data', ['filename', 'object'])\n","    gb = df.groupby(group)\n","    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n","\n","\n","def create_tf_example(group, label_map_dict, path):\n","    if tf.__version__>='2.0.0':\n","      with tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n","        encoded_jpg = fid.read()\n","    else:\n","      with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n","        encoded_jpg = fid.read()\n","\t\n","    encoded_jpg_io = io.BytesIO(encoded_jpg)\n","    image = Image.open(encoded_jpg_io)\n","    width, height = image.size\n","\n","    filename = group.filename.encode('utf8')\n","    image_format = b'jpg'\n","    # check if the image format is matching with your images.\n","    xmins = []\n","    xmaxs = []\n","    ymins = []\n","    ymaxs = []\n","    classes_text = []\n","    classes = []\n","\n","    for index, row in group.object.iterrows():\n","        xmins.append(row['xmin'] / width)\n","        xmaxs.append(row['xmax'] / width)\n","        ymins.append(row['ymin'] / height)\n","        ymaxs.append(row['ymax'] / height)\n","        classes_text.append(row['class'].encode('utf8'))\n","        classes.append(label_map_dict[row['class']]) \n","\n","    tf_example = tf.train.Example(features=tf.train.Features(feature={\n","        'image/height': dataset_util.int64_feature(height),\n","        'image/width': dataset_util.int64_feature(width),\n","        'image/filename': dataset_util.bytes_feature(filename),\n","        'image/source_id': dataset_util.bytes_feature(filename),\n","        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n","        'image/format': dataset_util.bytes_feature(image_format),\n","        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n","        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n","        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n","        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n","        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n","        'image/object/class/label': dataset_util.int64_list_feature(classes),\n","    }))\n","    return tf_example\n","\n","\n","def main():\n","    # Initiate argument parser\n","    parser = argparse.ArgumentParser(description=\"Sample TensorFlow XML-to-TFRecord converter\")\n","    parser.add_argument(\"-i\",\"--inputDir\",  help=\"Path to the folder where the input .xml files are stored\",type=str)\n","    parser.add_argument(\"-o\",\"--outputFile\",help=\"Name of output TFRecord file (including path)\", type=str)\n","    parser.add_argument(\"-l\",\"--labelFile\", help=\"Name of label map proto (including path)\", type=str)\n","    args = parser.parse_args()\n","\n","\t\n","    if(args.inputDir is None):\n","        args.inputDir = os.getcwd()\n","        \n","    assert(os.path.isdir(args.inputDir))\n","\n","    if '/' in args.inputDir:\n","        input_path_lst = args.inputDir.split('/')\n","    else:\n","        input_path_lst = args.inputDir.split('\\\\')\n","\t\n","    if ':' in input_path_lst[0]:\n","        to_workspace_path = os.path.join(input_path_lst[0],os.path.sep,*input_path_lst[1:-2])\n","    else:\n","        to_workspace_path = os.path.join(*input_path_lst[0:-2])\t\n","        \n","    if(args.outputFile is None):\n","        args.outputFile = os.path.join(to_workspace_path,\"annotations\",input_path_lst[-1]+\".record\")\n","\n","    if(args.labelFile is None):\n","        args.labelFile = os.path.join(to_workspace_path,\"annotations\",\"label_map.pbtxt\")\n","\n","\t\n","    xml_df = xml_to_csv(args.inputDir)\n","\n","    print(args.labelFile)\n","    label_map_dict = {label_name : idx+1 for idx, label_name in enumerate(sorted(xml_df['class'].unique()))}\n","\n","    label_map_item = \"item {{\\n    id: {0}\\n    name: '{1}'\\n}}\\n\"\t\n","\n","    with open(args.labelFile,'w') as fp:\n","        for label_name in sorted(label_map_dict.keys()):            \n","            fp.write(label_map_item.format(label_map_dict[label_name],label_name))\n","    \n","    if tf.__version__>='2.0.0':\t\n","      writer  = tf.io.TFRecordWriter(args.outputFile)\n","    else:\n","      writer  = tf.python_io.TFRecordWriter(args.outputFile)\n","\t\n","    grouped = split(xml_df, 'filename')    \n","    \n","    for group in grouped:\n","        tf_example = create_tf_example(group, label_map_dict, args.inputDir)\n","        writer.write(tf_example.SerializeToString())\n","\n","    writer.close()\n","\n","    print('Successfully created the TFRecords: {}'.format(args.outputFile))    \n","\n","if __name__ == '__main__':\n","    main()\n","</pre>"]},{"cell_type":"markdown","metadata":{"id":"RqWq7Z819rSQ"},"source":["### 步驟5:準備<a href=\"https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#configuring-a-training-pipeline\" title=\"config\" target=\"_blank\">設定檔</a>  \n","從<a href=\"https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\" target=\"_blank\" title=\"Model Zoo\">Tensorflow model zoo</a>挑選一個預先訓練好的模型，從那個模型開始訓練自己的物件偵測器。\n","<pre>\n","例如挑選ssd_mobilenet_v2_coco_2018_03_29.tar.gz\n","解壓縮後pre-training-model目錄下的樣子。\n","<img src=\"attachment:modelmask_pre-trained-model_folder.png\" width=\"250\">\n","</pre>\n","接下來修改pipeline.config然後放到training目錄內。\n","\n","#### 確認下面項目，是否要修改:\n","\n","+ num_classes: 2 #<font color=\"red\">此例為2</font>\n","+ train_config\n","  + batch_size: 6 #<font color=\"red\">自行斟酌</font>\n","+ fine_tune_checkpoint: \"pre-trained-model/model.ckpt\"\n","+ tain_input_reader\n","  + input_path: \"annotations/train.record\"\n","  + label_map_path: \"annotations/label_map.pbtxt\"\n","+ eval_input_reader\n","  + input_path: \"annotations/test.record\"\n","  + label_map_path: \"annotations/label_map.pbtxt\"  \n","  \n","#### 開始訓練前檢查清單\n","1. annotations目錄\n","  + label_map.pdtxt\n","  + test.record\n","  + train.record\n","2. training目錄\n","  + pipeline.config\n","3. pre-trained-model\n","<img src=\"attachment:modelmask_pre-trained-model_folder.png\" width=\"250\">\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dA_q294Z9rSS"},"outputs":[],"source":["label_map_item = \"item {{\\n    id: {0}\\n    name: '{1}'\\n}}\\n\"\n","print(label_map_item.format(1,'abc'))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"lab6_物件偵測Tensorflow Object Detection (1).ipynb","provenance":[],"collapsed_sections":[]},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}